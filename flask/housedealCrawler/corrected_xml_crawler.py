# corrected_xml_crawler.py
import requests
import xml.etree.ElementTree as ET
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class CorrectedXMLCrawler:
    """ì˜¬ë°”ë¥¸ XML íƒœê·¸ëª…ìœ¼ë¡œ ìˆ˜ì •ëœ í¬ë¡¤ëŸ¬"""

    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = 'http://apis.data.go.kr/1613000/RTMSDataSvcAptTrade'
        self.session = requests.Session()

        # âœ… ì‹¤ì œ APIì˜ ì˜ì–´ íƒœê·¸ëª… ë§¤í•‘
        self.tag_mapping = {
            # í•œê¸€ íƒœê·¸ëª… â†’ ì˜ì–´ íƒœê·¸ëª… (ì‹¤ì œ API ì‘ë‹µ)
            'ì•„íŒŒíŠ¸': 'aptNm',
            'ì¸µ': 'floor',
            'ë…„': 'dealYear',
            'ì›”': 'dealMonth',
            'ì¼': 'dealDay',
            'ì „ìš©ë©´ì ': 'excluUseAr',
            'ê±°ë˜ê¸ˆì•¡': 'dealAmount',
            'ì§€ë²ˆ': 'jibun',
            'ê±´ì¶•ë…„ë„': 'buildYear',
            'ë„ë¡œëª…': 'roadNm',
            'ë„ë¡œëª…ê±´ë¬¼ë³¸ë²ˆí˜¸ì½”ë“œ': 'roadNmBonbun',
            'ë„ë¡œëª…ê±´ë¬¼ë¶€ë²ˆí˜¸ì½”ë“œ': 'roadNmBubun',
            'ë²•ì •ë™': 'umdNm'
        }

    def _get_xml_text_corrected(self, element, korean_tag: str) -> str:
        """ìˆ˜ì •ëœ XML í…ìŠ¤íŠ¸ ì¶”ì¶œ - ì˜ì–´ íƒœê·¸ëª… ì‚¬ìš©"""
        # í•œê¸€ íƒœê·¸ëª…ì„ ì˜ì–´ íƒœê·¸ëª…ìœ¼ë¡œ ë³€í™˜
        english_tag = self.tag_mapping.get(korean_tag, korean_tag)

        # ì˜ì–´ íƒœê·¸ëª…ìœ¼ë¡œ ê²€ìƒ‰
        elem = element.find(english_tag)
        if elem is not None and elem.text:
            return elem.text.strip()

        # ì˜ì–´ íƒœê·¸ëª…ì´ ì•ˆë˜ë©´ í•œê¸€ íƒœê·¸ëª…ë„ ì‹œë„
        elem = element.find(korean_tag)
        if elem is not None and elem.text:
            return elem.text.strip()

        return ''

    def _get_xml_text_all_methods(self, element, korean_tag: str) -> str:
        """ëª¨ë“  ë°©ë²•ìœ¼ë¡œ XML í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„"""
        # ë°©ë²• 1: ì˜ì–´ íƒœê·¸ëª…
        english_tag = self.tag_mapping.get(korean_tag, korean_tag)
        elem = element.find(english_tag)
        if elem is not None and elem.text:
            return elem.text.strip()

        # ë°©ë²• 2: í•œê¸€ íƒœê·¸ëª…
        elem = element.find(korean_tag)
        if elem is not None and elem.text:
            return elem.text.strip()

        # ë°©ë²• 3: ëŒ€ì†Œë¬¸ì ë³€í˜• ì‹œë„
        for tag_variation in [english_tag.lower(), english_tag.upper(), english_tag]:
            elem = element.find(tag_variation)
            if elem is not None and elem.text:
                return elem.text.strip()

        return ''

    def crawl_region_data_corrected(self, region_code: str, target_month: str):
        """ìˆ˜ì •ëœ ì§€ì—­ ë°ì´í„° í¬ë¡¤ë§"""
        url = f"{self.base_url}/getRTMSDataSvcAptTrade"

        params = {
            'serviceKey': self.api_key,
            'pageNo': 1,
            'numOfRows': 10,  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 10ê±´ë§Œ
            'LAWD_CD': region_code,
            'DEAL_YMD': target_month
        }

        try:
            response = self.session.get(url, params=params, timeout=30)
            response.raise_for_status()

            # âœ… ë””ë²„ê¹…: ì „ì²´ ì‘ë‹µ ì¶œë ¥ (ì²˜ìŒ 1000ì)
            logger.info(f"ğŸ“‹ {region_code} XML ì‘ë‹µ ìƒ˜í”Œ:")
            logger.info(response.text[:1000] + "..." if len(response.text) > 1000 else response.text)

            root = ET.fromstring(response.content)

            # ê²°ê³¼ ì½”ë“œ í™•ì¸
            result_code = root.find('.//resultCode')
            if result_code is not None:
                code = result_code.text.strip()
                if code not in ['00', '000']:
                    logger.error(f"âŒ {region_code} API ì˜¤ë¥˜: {code}")
                    return []

            # item ìš”ì†Œ ì°¾ê¸°
            items = root.findall('.//item')

            if not items:
                logger.info(f"ğŸ“­ {region_code}: item ìš”ì†Œ ì—†ìŒ")
                return []

            logger.info(f"ğŸ“Š {region_code}: {len(items)}ê°œ item ë°œê²¬")

            # âœ… ì²« ë²ˆì§¸ itemì˜ ëª¨ë“  íƒœê·¸ ì¶œë ¥ (ë””ë²„ê¹…)
            if items:
                first_item = items[0]
                logger.info(f"ğŸ” ì²« ë²ˆì§¸ itemì˜ ëª¨ë“  íƒœê·¸:")
                for child in first_item:
                    logger.info(f"  {child.tag}: '{child.text}'")

            # ë°ì´í„° ì¶”ì¶œ (ìˆ˜ì •ëœ ë°©ë²•)
            deals = []
            for item in items:
                deal = {
                    'apt_nm': self._get_xml_text_all_methods(item, 'ì•„íŒŒíŠ¸'),
                    'floor': self._get_xml_text_all_methods(item, 'ì¸µ'),
                    'deal_year': self._get_xml_text_all_methods(item, 'ë…„'),
                    'deal_month': self._get_xml_text_all_methods(item, 'ì›”'),
                    'deal_day': self._get_xml_text_all_methods(item, 'ì¼'),
                    'exclu_use_ar': self._get_xml_text_all_methods(item, 'ì „ìš©ë©´ì '),
                    'deal_amount': self._get_xml_text_all_methods(item, 'ê±°ë˜ê¸ˆì•¡'),
                    'jibun': self._get_xml_text_all_methods(item, 'ì§€ë²ˆ'),
                    'build_year': self._get_xml_text_all_methods(item, 'ê±´ì¶•ë…„ë„'),
                }
                deals.append(deal)

                # âœ… ì²« ë²ˆì§¸ deal ë°ì´í„° ì¶œë ¥ (ë””ë²„ê¹…)
                if len(deals) == 1:
                    logger.info(f"ğŸ” ì²« ë²ˆì§¸ ë³€í™˜ëœ ë°ì´í„°:")
                    for key, value in deal.items():
                        logger.info(f"  {key}: '{value}'")

            logger.info(f"âœ… {region_code}: {len(deals)}ê±´ ì¶”ì¶œ ì™„ë£Œ")
            return deals

        except Exception as e:
            logger.error(f"âŒ {region_code} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return []


# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
def test_corrected_crawler():
    """ìˆ˜ì •ëœ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸"""
    API_KEY = "Tr5R4h7B1ooVneI7RvqRT7e/gww2bCjIovGfpJvwiEC3MgNNDCdPOwkfrx9UbXLZZtJedgWAl4mXCQNcD0tVAA=="

    crawler = CorrectedXMLCrawler(API_KEY)

    # ë°ì´í„°ê°€ ìˆì„ ê°€ëŠ¥ì„±ì´ ë†’ì€ ì¡°ê±´ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
    test_cases = [
        ('11680', '202408'),  # ê°•ë‚¨êµ¬ 2024ë…„ 8ì›”
        ('11650', '202407'),  # ì„œì´ˆêµ¬ 2024ë…„ 7ì›”
    ]

    for region_code, month in test_cases:
        logger.info(f"\n{'=' * 50}")
        logger.info(f"ğŸ§ª í…ŒìŠ¤íŠ¸: {region_code} {month}")
        logger.info('=' * 50)

        data = crawler.crawl_region_data_corrected(region_code, month)

        if data:
            logger.info(f"ğŸ‰ ì„±ê³µ: {len(data)}ê±´ ìˆ˜ì§‘")

            # ë°ì´í„° í’ˆì§ˆ í™•ì¸
            non_empty_data = [d for d in data if any(v.strip() for v in d.values())]
            logger.info(f"ğŸ“Š ë¹„ì–´ìˆì§€ ì•Šì€ ë°ì´í„°: {len(non_empty_data)}ê±´")

            if non_empty_data:
                logger.info("âœ… ìˆ˜ì •ëœ í¬ë¡¤ëŸ¬ ì„±ê³µ!")
                return True
        else:
            logger.warning(f"âš ï¸ {region_code} {month}: ë°ì´í„° ì—†ìŒ")

    logger.error("âŒ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
    return False


if __name__ == "__main__":
    test_corrected_crawler()
